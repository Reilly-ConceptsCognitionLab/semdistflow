---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# semdistflow

<!-- badges: start -->
<!-- badges: end -->

## Overview

‘semdistflow’ transforms any user-specified text into sequential bigrams (e.g. ‘The dog drinks the milk’ to dog-drink, drink-milk, etc.).  The package computes two measures of semantic distance for every running bigram in a language transcript.  Users have many options for how to structure their texts and tailor the output to their own unique constraints (e.g., omitting stopwords, lemmatizing tokens, dimensionality of word embeddings).

## Installation

You can install the development version of semdistflow from [GitHub](https://github.com/) by typing the following in your console or script (make sure you have devtools installed):

``` {r }
# install.packages("devtools")
devtools::install_github("Reilly-ConceptsCognitionLab/semdistflow")
```

## The main functions
readme() will look for any text files in your folder (preferably called my_texts) but you can specify your own folder name. 
<br />
<br />
cleanme() takes the dataframe from the readme step and does all sorts of cleaning and formatting operations to it.
<br />
<br />
distme() vectorizes the clean dataframe from the last step and comoputes two measures of semantic distance for every word pair


## Example of Cleaning Function

This is a basic example which shows you how the cleanme function works:

```{r example, message=FALSE}
library(semdistflow)
library(tidyverse)
doc_id <- "fox"
doc_text <- "The quick brown fox jumps over the lazy dog."
fox_text <-as.data.frame(cbind(doc_id,doc_text))
fox_text
```

```{r clean}
fox_clean <- cleanme(fox_text)
fox_clean
```

## Example of Semantic Distance Function

This is a basic example which shows you how the cleanme function works:

```{r token}
fox_token <-fox_clean %>%
  group_by(doc_id, doc_text) %>%
  tidytext::unnest_tokens(word, doc_clean, drop=F)
  fox_token$lemma<- textstem::lemmatize_words(fox_token$word)
fox_token
```



```{r semdist}
fox_dist <-  bigram_cos_sim(targetdf = fox_token, lookupdb = semdist15, colname1 = lemma, colname2 = word, flipped = T)

fox_dist
```

```{r plot}
ggplot(fox_dist, aes(x=as.numeric(row.names(fox_dist)), y=flipped_cosine.dist)) +  geom_line(color="#02401BD9", size= 1) + theme_classic() + xlab(NULL) + ylab(NULL)  + geom_label(aes(label=pair), size=3, data=fox_dist)  
```
