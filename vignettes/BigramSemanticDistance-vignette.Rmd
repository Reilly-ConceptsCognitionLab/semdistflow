---
title: "BigramSemanticDistance-vignette"
author: "Bonnie Zuckerman & Jamie Reilly"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{BigramSemanticDistance-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE, warning=FALSE}
remotes::install_github("Reilly-ConceptsCognitionLab/semdistflow")
library(semdistflow)
library(tidytext)
library(tidyverse)
library(knitr)
library(kableExtra)
library(stringr)
library(zoo)
library(stringi)
library(printr)
```

# Preparing Your Texts

Each text you wish to process should be saved as a .txt file. Your text can be stored in one folder or broken up into multiple folders. The file names in a given folder should be unique. 

# Loading your texts

```{r load example, eval=FALSE}
mytexts <- readtxt(".") #reads all .txt in the working directory 
mytextsinfolder <- readtxt("./your-text-folder") #reads all texts in a given folder in the working directory 
```

```{r load, message=FALSE, warning=FALSE, echo=FALSE}
folder.path <- system.file("extdata", package = "semdistflow", mustWork = TRUE)
mytext <- readtxt(folder.path)
```

The resulting data.frame should have two columns "doc_id" and "doc_text". 

```{r, raw table print, echo=FALSE}
print(mytext)
```


If you wish to use this package but want to read in your data another way, you must ensure that you have the columns names "doc_id" and "doc_text". The "doc_id" column but be unique for each text. You may include other columns if you wish. 

# "Cleaning" your data 

```{r clean,}
mytext.clean <- cleanme(mytext)
```

```{r, clean table print, echo=FALSE}
head(mytext.clean)
```

# Tokenize and lemmatize your data

There are many ways to tokenize or lemmatize your data. Here we use the package tidy text to tokenize our data and textstem to lemmatize it. 

```{r token,}
mytext.token<- mytext.clean %>%
  unnest_tokens(word, doc_clean)

mytext.token$lemma<- textstem::lemmatize_words(mytext.token$word)

```

```{r, lemma table print, echo=FALSE}
head(mytext.token)
```

# Between Word Distance Calculations 

## Using GloVe embeddings

```{r glove, message=FALSE}
mytext.glove <- bigram_cos_sim(targetdf = mytext.token, lookupdb = wiki_model_100, colname1 = lemma, colname2 = word)
```

```{r, glove table print, echo=FALSE}
head(mytext.glove)
```

## Using SemDist15 embeddings

```{r, semdist15, message=FALSE}
mytext.semdist <- bigram_cos_sim(targetdf = mytext.token, lookupdb = semdist15, colname1 = lemma, colname2 = word)
```

```{r, semdist15 table print, echo=FALSE}
head(mytext.semdist)
```

# Plot

```{r}
library(stringr)
mytext.glove <- mytext.glove %>% 
  mutate(text_name = sub('.*\\/', '', doc_id))

mytext.glove<- mytext.glove %>%
  group_by(doc_id) %>% 
  mutate(id = row_number())


theme_plot <- ggplot(mytext.glove, aes(x = id, y=cosine.dist)) +  geom_line(color="#02401BD9") + facet_grid(~text_name, scales="free")

theme_plot
```

```{r}
mytext.semdist <- mytext.semdist %>% 
  mutate(text_name = sub('.*\\/', '', doc_id))

mytext.semdist<- mytext.semdist %>%
  group_by(doc_id) %>% 
  mutate(id = row_number())


tax_plot <- ggplot(mytext.semdist, aes(x = id, y=cosine.dist)) +  geom_line(color="#EBCC2AA3") + facet_grid(~text_name, scales="free")

tax_plot
```





